{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacb15d4",
   "metadata": {},
   "source": [
    "# BatchNorm和LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8de6a",
   "metadata": {},
   "source": [
    "🚩目录\n",
    "    ①二维数据的BatchNorm和LayerNorm\n",
    "    ②图像数据的BatchNorm和LayerNorm\n",
    "    ③文本数据的BatchNorm和LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c00d38",
   "metadata": {},
   "source": [
    "   ###  ① 二维数据的BatchNorm和LayerNorm\n",
    "   假设有 **二维数据 X**:\n",
    "   Batchsize为3，Feature个数为4（每个样本由四个特征构成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180f6dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float).reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561f989",
   "metadata": {},
   "source": [
    "看看概念：\n",
    "\n",
    "   BatchNorm是对这个batch数据中的**每个特征**进行样本间的归一化（也就是上面的X进行纵向的归一化\n",
    "   \n",
    "   LayerNorm是对**每个样本**在其特征维度上进行归一化（上面的X进行横向归一化\n",
    "   \n",
    "操作和结果如下：(注意观察mean和var的形状)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a384d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 5., 6., 7.]]),\n",
       " tensor([[16., 16., 16., 16.]]),\n",
       " tensor([[-1., -1., -1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  1.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求均值和方差\n",
    "batch_mean = torch.mean(X, dim=0, keepdim=True)\n",
    "batch_var = torch.var(X, dim=0, keepdim=True)  # 计算样本方差除以n-1而不是n\n",
    "# 计算\n",
    "batch_X = (X - batch_mean)/torch.sqrt(batch_var)\n",
    "batch_mean, batch_var, batch_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56076b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.5000],\n",
       "         [5.5000],\n",
       "         [9.5000]]),\n",
       " tensor([[1.6667],\n",
       "         [1.6667],\n",
       "         [1.6667]]),\n",
       " tensor([[-1.1619, -0.3873,  0.3873,  1.1619],\n",
       "         [-1.1619, -0.3873,  0.3873,  1.1619],\n",
       "         [-1.1619, -0.3873,  0.3873,  1.1619]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求均值和方差\n",
    "layer_mean = torch.mean(X, dim=1, keepdim=True)\n",
    "layer_var = torch.var(X, dim=1, keepdim=True)\n",
    "# 计算\n",
    "layer_X = (X - layer_mean)/torch.sqrt(layer_var)\n",
    "layer_mean, layer_var, layer_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85410b",
   "metadata": {},
   "source": [
    " ###  ② 图像数据的BatchNorm和LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb6731",
   "metadata": {},
   "source": [
    "图像数据有四个维度 (B, C, H, W),分别是 batch大小，通道数量，高， 宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "172d8dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 2, 2]),\n",
       " tensor([[[[ 0.,  1.],\n",
       "           [ 2.,  3.]],\n",
       " \n",
       "          [[ 4.,  5.],\n",
       "           [ 6.,  7.]],\n",
       " \n",
       "          [[ 8.,  9.],\n",
       "           [10., 11.]]],\n",
       " \n",
       " \n",
       "         [[[12., 13.],\n",
       "           [14., 15.]],\n",
       " \n",
       "          [[16., 17.],\n",
       "           [18., 19.]],\n",
       " \n",
       "          [[20., 21.],\n",
       "           [22., 23.]]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X表示一个batch中的两张三通道图片，图片的高和宽都为两个像素\n",
    "X = torch.arange(24, dtype=torch.float).reshape(2, 3, 2, 2)\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b8578",
   "metadata": {},
   "source": [
    "BatchNorm和LayerNorm的定义还是不变的，重点在找到特征维度\n",
    "\n",
    "如一张RGB图片由红绿蓝三个通道组成，那么每一个通道就是这个图片的一个特征，只不过每一个特征都是二维张量\n",
    "\n",
    "所以C这一个维度就是特征维度\n",
    "\n",
    "#### 2.1 图片的BatchNorm\n",
    "BatchNorm的方向类似于："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f519ab8",
   "metadata": {},
   "source": [
    "<img src=\"..\\Apictures\\2.jpg\" alt=\"2\" style=\"zoom:15%;\" />\n",
    "\n",
    "所以生成的mean和var的数量和C也就是特征的数量是一样的，现在你再回去读下BatchNorm的定义，会有更直观的理解。\n",
    "\n",
    "下面我们用pytorch中自带的类来演示图片的BatchNorm操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "900e77c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里我们直接调用pytorch中的BatchNorm2d来进行处理\n",
    "bn2d = torch.nn.BatchNorm2d(num_features=3)\n",
    "bn2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f68ee9",
   "metadata": {},
   "source": [
    "<img src=\"..\\Apictures\\1.png\" alt=\"1\" style=\"zoom:50%;\" />\n",
    "这个公式便是BatchNorm2d所做的事情\n",
    "\n",
    "除了归一化，还有γ和β可以对标准正态分布进行偏移（变换方差和均值），初始化分别为1和0。这两个参数是可以学习的，最终是多少由训练过程决定，而不是我们指定。当然我们也不是什么都不能做，我们可以通过在实例化对象时指定affine=False让这两个参数变成不可学习的，那么它俩就固定为1和0了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c30539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight和bias就是指γ和β，它俩的长度和mean,var的对数(这个对数不是指log!)相同，也就是和C相同，\n",
    "# 所以实例化bn2d对象时要传入C，进行weight和bias的初始化\n",
    "bn2d.weight, bn2d.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1cb9e",
   "metadata": {},
   "source": [
    "参数解释：\n",
    "- num_features:特征/通道的数量，因为batchnorm实例化需要知道γ和β的形状，他俩的形状和C一样，所以在实例化该对象的时候需要传入C。\n",
    "- momentum=0.1:默认0.1，用于学习过程中更新γ和β，具体更新公式如下：\n",
    "<img src=\"..\\Apictures\\3.png\" alt=\"3\" style=\"zoom:25%;\" />\n",
    "- affine=True:控制γ和β是否可以学习，默认可以学习\n",
    "- eps:公式中的ε，防止分母为0\n",
    "- tracking_running_stats:控制是否要更新running_mean和running_var,这两个变量用作模型eval的mean和var,如果tracking_running_stats设置为false，那么这两个变量都为None，模型eval时直接根据数据计算mean和var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcb3b34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.2288, -1.0650],\n",
       "          [-0.9012, -0.7373]],\n",
       "\n",
       "         [[-1.2288, -1.0650],\n",
       "          [-0.9012, -0.7373]],\n",
       "\n",
       "         [[-1.2288, -1.0650],\n",
       "          [-0.9012, -0.7373]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7373,  0.9012],\n",
       "          [ 1.0650,  1.2288]],\n",
       "\n",
       "         [[ 0.7373,  0.9012],\n",
       "          [ 1.0650,  1.2288]],\n",
       "\n",
       "         [[ 0.7373,  0.9012],\n",
       "          [ 1.0650,  1.2288]]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm_X = bn2d(X)\n",
    "batchnorm_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082262b",
   "metadata": {},
   "source": [
    "用上面数据进行验证：\n",
    "\n",
    "取两个样本的第一个特征[-1.2288, -1.0650, -0.9012, -0.7373]和[ 0.7373,  0.9012, 1.0650,  1.2288],相加的确等于0，说明服从标准正太分布\n",
    "\n",
    "其余特征这里就不验证了，可自行相加验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9efc21",
   "metadata": {},
   "source": [
    "#### 2.2 图片的LayerNorm\n",
    "LayerNorm的方向类似于：\n",
    "<img src=\"..\\Apictures\\7.jpg\" alt=\"7\" style=\"zoom:15%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0056e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LayerNorm((3, 2, 2), eps=1e-05, elementwise_affine=True),\n",
       " torch.Size([3, 2, 2]),\n",
       " torch.Size([3, 2, 2]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里我们直接调用pytorch中的LayerNorm来进行处理\n",
    "layer_norm = torch.nn.LayerNorm(normalized_shape=[3, 2, 2])\n",
    "layer_norm, layer_norm.weight.shape, layer_norm.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a9f5a",
   "metadata": {},
   "source": [
    "需要说明的是，虽然对每一个样本计算一对mean和var（在这个例子中总共两对），但是layerNorm的γ和β却不止有两个，而是element-wise的。\n",
    "\n",
    "具体来说，对于一个最基本的像素点和它在其它样本中相同位置的所有像素点，分配一对γ和β。是这样的，但是为什么要这样设计，就要问layernorm的提出者了。\n",
    "\n",
    "跟batchnorm相同，layernorm实例化的时候要初始化weights和bias,也就是γ和β的形状，所以需要传入normalized_shape\n",
    "\n",
    "<img src=\"..\\Apictures\\6.png\" alt=\"6\" style=\"zoom:50%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3ea2583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5933, -1.3036],\n",
       "          [-1.0139, -0.7242]],\n",
       "\n",
       "         [[-0.4345, -0.1448],\n",
       "          [ 0.1448,  0.4345]],\n",
       "\n",
       "         [[ 0.7242,  1.0139],\n",
       "          [ 1.3036,  1.5933]]],\n",
       "\n",
       "\n",
       "        [[[-1.5933, -1.3036],\n",
       "          [-1.0139, -0.7242]],\n",
       "\n",
       "         [[-0.4345, -0.1448],\n",
       "          [ 0.1448,  0.4345]],\n",
       "\n",
       "         [[ 0.7242,  1.0139],\n",
       "          [ 1.3036,  1.5933]]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm_X = layer_norm(X)\n",
    "layernorm_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73895ed8",
   "metadata": {},
   "source": [
    " ###  ③  文本数据的BatchNorm和LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949d47a",
   "metadata": {},
   "source": [
    "在transformer及rnn这些语言模型中处理的文本向量一般形状为：     (B, T, H)\n",
    "\n",
    "其中，\n",
    "- B是batch_size，也就是这个batch中有几句话\n",
    "- T可以理解为每句话中的单词数量\n",
    "- H可以理解为每一个单词被表示为几维的向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae3d16e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0364,  0.4676, -0.6561, -0.7976, -0.0697],\n",
       "         [ 0.1216,  0.9560,  1.5325,  0.0071, -1.0951],\n",
       "         [-0.0868,  0.4512,  0.1544,  0.3392,  0.5351]],\n",
       "\n",
       "        [[-0.2735,  1.5836, -0.6444,  0.2002, -0.9999],\n",
       "         [ 0.6813,  1.1774, -1.6905,  1.8488,  0.3426],\n",
       "         [ 0.8175,  0.0263, -0.7140, -1.4588,  0.0625]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn([2, 3, 5])\n",
    "X # 每一行可以看作一个字，每三行表示一个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2060a0",
   "metadata": {},
   "source": [
    "transformer中用的是layernorm，所以现在大多数处理文本的模型用的都是layernorm，所以这里就只演示layernorm\n",
    "\n",
    "（其实是对batchnorm1d有些魔迷惑，所以就不误导读者了哈哈哈哈）\n",
    "\n",
    "这里用的layerNorm跟图片用的layernorm是一摸一样的，只是传入的normalized_shape不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d36c727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LayerNorm((3, 5), eps=1e-05, elementwise_affine=True),\n",
       " torch.Size([3, 5]),\n",
       " torch.Size([3, 5]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm = torch.nn.LayerNorm(normalized_shape=[3,5])\n",
    "layer_norm, layer_norm.weight.shape, layer_norm.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b68c3f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5503e+00,  5.8628e-01, -1.0100e+00, -1.2111e+00, -1.7694e-01],\n",
       "         [ 9.4729e-02,  1.2803e+00,  2.0992e+00, -6.7840e-02, -1.6337e+00],\n",
       "         [-2.0130e-01,  5.6310e-01,  1.4138e-01,  4.0390e-01,  6.8228e-01]],\n",
       "\n",
       "        [[-3.3161e-01,  1.4935e+00, -6.9610e-01,  1.3389e-01, -1.0455e+00],\n",
       "         [ 6.0668e-01,  1.0943e+00, -1.7242e+00,  1.7541e+00,  2.7389e-01],\n",
       "         [ 7.4060e-01, -3.6963e-02, -7.6450e-01, -1.4965e+00, -1.4377e-03]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm_X = layer_norm(X)\n",
    "layernorm_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe808f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3842e-07,  1.1921e-07], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ = torch.sum(layernorm_X,dim=[1, 2])\n",
    "summ "
   ]
  },
  {
   "cell_type": "raw",
   "id": "446eca6a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
